import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

INPUT_FILE = "pinterest_clean.csv"

def train_model():
    df = pd.read_csv(INPUT_FILE)

    #features (texto) e target (repins)
    X = df["text"].fillna("")
    y = df["repin_count"]

    #vetorizaÃ§Ã£o TF-IDF
    vectorizer = TfidfVectorizer(max_features=500)
    X_tfidf = vectorizer.fit_transform(X)

    #dividir em treino-teste
    X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

    #regressÃ£o linear
    model = LinearRegression()  
    model.fit(X_train, y_train)

    #previsÃµes
    y_pred = model.predict(X_test)

    #avaliaÃ§Ã£o
    print("ğŸ“‰ MSE:", mean_squared_error(y_test, y_pred))
    print("ğŸ“Š RÂ²:", r2_score(y_test, y_pred))

    # Obter nomes das features (palavras) e coeficientes
    feature_names = vectorizer.get_feature_names_out()
    coefs = model.coef_

    # Top 10 palavras mais â€œpositivasâ€ (aumentam repins previstos)
    top_pos = sorted(zip(coefs, feature_names), reverse=True)[:10]

    # Top 10 palavras mais â€œnegativasâ€
    top_neg = sorted(zip(coefs, feature_names))[:10]

    print("\nğŸ’¡ Palavras associadas a MAIS repins:", top_pos)
    print("\nâš ï¸ Palavras associadas a MENOS repins:", top_neg)

    #grÃ¡fico real vs. previsÃ£o
    plt.scatter(y_test, y_pred, alpha=0.5)
    plt.xlabel("Repins reais")
    plt.ylabel("Repins previstos")
    plt.title("ComparaÃ§Ã£o entre Repins Reais e Previstos")
    plt.show()

if __name__ == "__main__":
    train_model()